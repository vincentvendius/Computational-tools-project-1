{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Customer Database\n",
    "**This is the first of three mandatory projects to be handed in as part of the assessment for the course 02807 Computational Tools for Data Science at Technical University of Denmark, autumn 2019.**\n",
    "\n",
    "#### Practical info\n",
    "- **The project is to be done in groups of at most 3 students**\n",
    "- **Each group has to hand in _one_ Jupyter notebook (this notebook) with their solution**\n",
    "- **The hand-in of the notebook is due 2019-10-13, 23:59 on DTU Inside**\n",
    "\n",
    "#### Your solution\n",
    "- **Your solution should be in Python**\n",
    "- **For each question you may use as many cells for your solution as you like**\n",
    "- **You should document your solution and explain the choices you've made (for example by using multiple cells and use Markdown to assist the reader of the notebook)**\n",
    "- **You should not remove the problem statements, and you should not modify the structure of the notebook**\n",
    "- **Your notebook should be runnable, i.e., clicking [>>] in Jupyter should generate the result that you want to be assessed**\n",
    "- **You are not expected to use machine learning to solve any of the exercises**\n",
    "- **You will be assessed according to correctness and readability of your code, choice of solution, choice of tools and libraries, and documentation of your solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Your team has been hired by the company X as data scientists. X makes gadgets for a wide range of industrial and commercial clients.\n",
    "\n",
    "As in-house data scientists, your teams first task, as per request from your new boss, is to optimize business operations. You have decided that a good first step would be to analyze the companys historical sales data to gain a better understanding of where profit is coming from. It may also reveal some low hanging fruit in terms of business opportunities.\n",
    "\n",
    "To get started, you have called the IT department to get access to the customer and sales transactions database. To your horror you've been told that such a database doens't exist, and the only record of sales transactions is kept by John from finance in an Excel spreadsheet. So you've emailed John asking for a CSV dump of the spreadsheet...\n",
    "\n",
    "In this project you need to clean the data you got from John, enrich it with further data, prepare a database for the data, and do some data analysis. The project is comprised of five parts. They are intended to be solved in the order they appear, but it is highly recommended that you read through all of them and devise an overall strategy before you start implementing anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Cleaning the data\n",
    "John has emailed you the following link to the CSV dump you requested.\n",
    "\n",
    "- [transactions.csv](https://raw.githubusercontent.com/patrickcording/02807-comp-tools/master/docker/work/data/transactions.csv)\n",
    "\n",
    "It seems as though he has been a bit sloppy when keeping the records. \n",
    "\n",
    "In this part you should:\n",
    "- Explain what the data is\n",
    "- Clean it to prepare it for inserting into a database and doing data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CSV file given to us by John from finances, a large table can be found. \n",
    "\n",
    "Each row represents a transaction from a sale of gadgets to a company. \n",
    "\n",
    "The file is loaded to get more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part</th>\n",
       "      <th>company</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54868-5165</td>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>784.79€</td>\n",
       "      <td>2016-01-02 00:01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60505-2867</td>\n",
       "      <td>Lajo</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Thessaloniki</td>\n",
       "      <td>187.99€</td>\n",
       "      <td>2016-01-02 00:05:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24385-268</td>\n",
       "      <td>Flipstorm</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "      <td>221.73€</td>\n",
       "      <td>2016-01-02 00:18:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         part        company country          city    price  \\\n",
       "0  54868-5165  Chatterbridge   Spain     Barcelona  784.79€   \n",
       "1  60505-2867           Lajo  Greece  Thessaloniki  187.99€   \n",
       "2   24385-268      Flipstorm  Greece        Athens  221.73€   \n",
       "\n",
       "                  date  \n",
       "0  2016-01-02 00:01:05  \n",
       "1  2016-01-02 00:05:26  \n",
       "2  2016-01-02 00:18:30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('transactions.csv', encoding='utf-8-sig')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20568"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_rows = len(df)\n",
    "start_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table consists of 20568 data rows and there are 6 columns of information for each transaction. \n",
    "\n",
    "The first column, 'part', is not unique nor does it give information important in regards to doing this exercise. Therefore, it is removed from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>784.79€</td>\n",
       "      <td>2016-01-02 00:01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lajo</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Thessaloniki</td>\n",
       "      <td>187.99€</td>\n",
       "      <td>2016-01-02 00:05:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flipstorm</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "      <td>221.73€</td>\n",
       "      <td>2016-01-02 00:18:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         company country          city    price                 date\n",
       "0  Chatterbridge   Spain     Barcelona  784.79€  2016-01-02 00:01:05\n",
       "1           Lajo  Greece  Thessaloniki  187.99€  2016-01-02 00:05:26\n",
       "2      Flipstorm  Greece        Athens  221.73€  2016-01-02 00:18:30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['part'], inplace=True, axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a better knowledge of the type of data, the first order of business is to have python tell us what type of data we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company    object\n",
       "country    object\n",
       "city       object\n",
       "price      object\n",
       "date       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output tells us that the data in all the columns is of 'object' type - also known as 'string' type. However, from the table it is clear that the data in the 'price' and 'date' columns should not be an object but rather a float and datetime, respectively. \n",
    "\n",
    "Now when we know what the data is, we need to make a plan for how to clean it:\n",
    "\n",
    "- Make sure that all dates in the 'date' column are real dates and in the same format.\n",
    "- Remove all the NaN values from company, country and city.\n",
    "- Clean the 'price' column by removing rows without a price and make sure all currencies are in euro. \n",
    "- Remove duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that all dates in the 'date' column are real dates and in the same format.\n",
    "\n",
    "***I:*** We find the transactions with the default date format. If a transaction is not in the default format it is changed to it. Further, if a date is a non-valid date it is removed (e.g. 2017-13-32). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_range = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    try:\n",
    "        pd.to_datetime(df.iloc[i,4], format='%Y-%m-%d')\n",
    "    except:\n",
    "        try: \n",
    "            df.iloc[i,4] = pd.to_datetime(df.iloc[i,4])\n",
    "        except:\n",
    "            out_of_range.append(i)\n",
    "\n",
    "df.drop(out_of_range, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***II:*** We change the dtype from object to datetime64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company            object\n",
       "country            object\n",
       "city               object\n",
       "price              object\n",
       "date       datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = df['date'].astype('datetime64[ns]')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SUCCES!*** All dates are now changed to the same format and the dtype is changed to datetime64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all the NaN values from company, country and city.\n",
    "\n",
    "***I:*** We want to make sure that all countries in the country column is real countries. To do this we extract an updated json-file from the worldbank data and collect all real countries in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://api.worldbank.org/v2/sources/2/country/all/data?per_page=500&format=json&mrnev=1')\n",
    "json_response = r.json()\n",
    "res = json_response['source']\n",
    "\n",
    "list_country = []\n",
    "\n",
    "for r in res:    \n",
    "    inner_res = r.get('concept')\n",
    "    for r in inner_res:\n",
    "        country = r.get('variable')\n",
    "        for r in country:\n",
    "            country = r.get('value')\n",
    "            list_country.append(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after we generated the list it is easy to see which country is not in the list and thus is suspicious. These should be taken a closer look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 'Tyskland', 'Portuga', 'US']\n"
     ]
    }
   ],
   "source": [
    "not_real = []\n",
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i,1] not in list_country:\n",
    "        not_real.append(df.iloc[i,1])\n",
    "\n",
    "print(list(set(not_real)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly realize that John from finances made some typos here and manually change these to the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i,1] == 'US':\n",
    "        df.iloc[i,1] = 'United States'\n",
    "    elif df.iloc[i,1] == 'Tyskland':\n",
    "        df.iloc[i,1] = 'Germany'\n",
    "    elif df.iloc[i,1] == 'Portuga':\n",
    "        df.iloc[i,1] = 'Portugal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When John from finances clearly could make typos in the country-column we now wonder whether he also can do the same thing in the company-column. Since there is not database over imagnianary comapnies we have to do something else. Instead we count the number of times the company occurs in the data frame and make the assumption that any company with a representation below or equal to 5 have to be looked closelyer upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies that is underrepresented: [' -', ' a', 'Laj0', 'Ntagz', 'Thoughtmixz', 'Zooxo.', 'aa']\n"
     ]
    }
   ],
   "source": [
    "counts = dict()\n",
    "weird_company = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i,0] in counts:\n",
    "        counts[df.iloc[i,0]] += 1\n",
    "    else:\n",
    "        counts[df.iloc[i,0]] = 1    \n",
    "\n",
    "for char in sorted(counts.keys()):\n",
    "    if counts[char] <= 5:\n",
    "        weird_company.append(char)\n",
    "\n",
    "print('Companies that is underrepresented: %s' % weird_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a quick glance at the 'true' companies we see that trully enough most of these were typos. These are corrected. Three companies still look weird (' -', ' a' and 'aa') and we wonder why. We will leave them be for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i,0] == 'Zooxo.':\n",
    "        df.iloc[i,0] = 'Zooxo'\n",
    "    elif df.iloc[i,0] == 'Thoughtmixz':\n",
    "        df.iloc[i,0] = 'Thoughtmix'\n",
    "    elif df.iloc[i,0] == 'Ntagz':\n",
    "        df.iloc[i,0] = 'Ntags'\n",
    "    elif df.iloc[i,0] == 'Laj0':\n",
    "        df.iloc[i,0] = 'Lajo' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we group each company by its countries. We assume that three thing will happen now. (I): There is only one country were this company is present. In that case nothing is done. (II): There is one country and one NaN value for the company. Here we replace the NaN value with the specific country. (III): There are multiple countries were this country is present (and possible also NaN values). This is an odd case and we have to look closer at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries that has to be investigated further: ['Flipstorm']\n"
     ]
    }
   ],
   "source": [
    "grouped = df.groupby('company')['country'].unique().apply(list).to_dict()\n",
    "\n",
    "still_weird_company = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    if pd.isna(df.iloc[i,1]) == True:\n",
    "        if len(grouped[df.iloc[i,0]]) == 2:\n",
    "            df.iloc[i,1] = grouped[df.iloc[i,0]][0]\n",
    "        elif len(grouped[df.iloc[i,0]]) >= 2:\n",
    "            still_weird_company.append(df.iloc[i,0])\n",
    "            \n",
    "print('Countries that has to be investigated further: %s' % list(set(still_weird_company)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SUCCES!*** All companies (except 'Flipstorm') now only have one country and no NaN value should be present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we group each company by its countries and city. We assume that three thing will happen now. (I): There is only one country and one city were this company is present. In that case nothing is done. (II): There is one country, one city and one NaN value for the company. In that case nothing is done just yet. (III): There are multiple countries or multiple cities were this country is present (and possible also NaN values). This is an odd case and we have to look closer at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Greece', 'Athens', 'France', 'Nanterre', nan], ['Portugal', 'Braga', nan, 'Monção'], ['Portugal', 'Amadora\\t', 'Vila Fria', nan]]\n"
     ]
    }
   ],
   "source": [
    "grouped = (df.groupby(['company'])['country', 'city']\n",
    "       .apply(lambda x: pd.unique(x.values.ravel()).tolist()))\n",
    "\n",
    "weird_city = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    if len(grouped[df.iloc[i,0]]) > 3:\n",
    "        if grouped[df.iloc[i,0]] in weird_city:\n",
    "            pass\n",
    "        else:\n",
    "            weird_city.append(grouped[df.iloc[i,0]])\n",
    "\n",
    "print(weird_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see three cases that is weird. But after a quick google search we realize that John from finances once again made a mistake. Vila Fria is not a city but an area in Amadora and thus we changed it to Amadora. Monção is neither a city by an area as well. Monção is thus changed to Braga. Note: We also removed the extra /t from Amadora. \n",
    "\n",
    "Furthermore; If the country is Greece the city should be Athens, and vice versa. If the country is France the city should be Nanterre, and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i,2] == 'Amadora\\t':\n",
    "        df.iloc[i,2] = 'Amadora'\n",
    "    elif df.iloc[i,2] == 'Vila Fria':\n",
    "        df.iloc[i,2] = 'Amadora'\n",
    "    elif df.iloc[i,2] == 'Monção':\n",
    "        df.iloc[i,2] = 'Braga'\n",
    "        \n",
    "    elif df.iloc[i,1] == 'Greece':\n",
    "        df.iloc[i,2] = 'Athens'\n",
    "    elif df.iloc[i,1] == 'France':\n",
    "        df.iloc[i,2] = 'Nanterre'\n",
    "    elif df.iloc[i,2] == 'Athens':\n",
    "        df.iloc[i,1] = 'Greece'\n",
    "    elif df.iloc[i,2] == 'Nanterre':\n",
    "        df.iloc[i,1] = 'France'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you fixed this, you once again group them together. This time you focus on all the cases were there is one country, one city and one NaN value for the company. Here we replace the NaN value with the specific city since we now the NaN value cannot be the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (df.groupby(['company'])['country', 'city']\n",
    "       .apply(lambda x: pd.unique(x.values.ravel()).tolist()))\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    if pd.isna(df.iloc[i,2]) == True:\n",
    "        if len(grouped[df.iloc[i,0]]) == 3:\n",
    "            df.iloc[i,2] = grouped[df.iloc[i,0]][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sight of relieve leaves your body. 'Done' you think. But suddenly you remeber the three weird companies 'aa', ' a' and ' -'. You wonder if John's typo is so servere that it actually is not a mistake. To check this, you see if the placement of the company is identical to some of the other companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The company placement of \"aa\" matches the company \"Wordify\".\n",
      "The company placement of \" a\" matches the company \"Wordify\".\n",
      "The company placement of \" -\" matches the company \"Zoonder\".\n"
     ]
    }
   ],
   "source": [
    "grouped = (df.groupby(['company'])['country', 'city']\n",
    "       .apply(lambda x: pd.unique(x.values.ravel()).tolist()))\n",
    "\n",
    "weird_company = ['aa',' a',' -']\n",
    "\n",
    "for j in weird_company:\n",
    "    lol = grouped[j]\n",
    "    for i in range(len(grouped)):\n",
    "        if grouped[i] == lol and grouped.keys()[i] not in weird_company:\n",
    "            print('The company placement of \"%s\" matches the company \"%s\".' % (j, grouped.keys()[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each companies location only matches one other company and we thus make the assumption that these companies are indeed the same as the once at the same location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i,0] == 'aa':\n",
    "        df.iloc[i,0] = 'Wordify'\n",
    "    elif df.iloc[i,0] == ' a':\n",
    "        df.iloc[i,0] = 'Wordify'\n",
    "    elif df.iloc[i,0] == ' -':\n",
    "        df.iloc[i,0] = 'Zoonder'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do a final status overview to check if everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company\n",
       "Avaveo                           [France, Nanterre]\n",
       "Brainsphere                       [Portugal, Braga]\n",
       "Bubblemix                            [Japan, Asaka]\n",
       "Buzzbean                      [Germany, Düsseldorf]\n",
       "Chatterbridge                    [Spain, Barcelona]\n",
       "Eimbee                           [France, Nanterre]\n",
       "Flipstorm        [Greece, Athens, France, Nanterre]\n",
       "Gabcube                          [Portugal, Almada]\n",
       "Gabtune                          [France, Nanterre]\n",
       "Gevee                            [France, Nanterre]\n",
       "Innojam                    [Netherlands, Amsterdam]\n",
       "Kanoodle                           [Japan, Niihama]\n",
       "Lajo                               [Greece, Athens]\n",
       "Ntags                            [Portugal, Lisbon]\n",
       "Realpoint                        [Portugal, Lisbon]\n",
       "Rhycero                          [France, Nanterre]\n",
       "Riffpath                           [Greece, Athens]\n",
       "Roodel                          [Portugal, Aranhas]\n",
       "Shufflebeat                       [Portugal, Porto]\n",
       "Tagtune                       [Switzerland, Zürich]\n",
       "Teklist                       [Netherlands, Arnhem]\n",
       "Thoughtmix                      [Portugal, Amadora]\n",
       "Twitterbeat                      [France, Nanterre]\n",
       "Voomm                            [France, Nanterre]\n",
       "Wordify                   [United States, New York]\n",
       "Yozio                              [Greece, Athens]\n",
       "Zoonder                     [United States, Boston]\n",
       "Zooxo                      [United Kingdom, London]\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = (df.groupby(['company'])['country', 'city']\n",
    "       .apply(lambda x: pd.unique(x.values.ravel()).tolist()))\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weird feeling creep upon you. Yesterday in the news you saw that companies cheated in taxes by placing there company in non-existing cities. GASP! To ease you fear you quickly make a script that can check if the respective city is placed in the correct land. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Germany': 'Düsseldorf', 'Portugal': 'Aranhas', 'Switzerland': 'Zürich'}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('https://raw.githubusercontent.com/russ666/all-countries-and-cities-json/6ee538beca8914133259b401ba47a550313e8984/countries.json')\n",
    "json_response = r.json()\n",
    "res = json_response\n",
    "res\n",
    "\n",
    "weird_countries = {}\n",
    "\n",
    "country_list = {}\n",
    "\n",
    "for r in res:\n",
    "    country_list[r] = res[r]\n",
    "    \n",
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i,1] in country_list.keys():\n",
    "        country = str(df.iloc[i,1])\n",
    "        if df.iloc[i,2] not in country_list[country]:\n",
    "            weird_countries[df.iloc[i,1]] = df.iloc[i,2]\n",
    "\n",
    "print(weird_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After quick look (and a google search) you mind is at rest again. All the above is true city in its respective country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SUCCES!*** Now there are no more NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD TEXT!!! :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dream scenario of having a company in only one country and one city is far from fulfilled. \n",
    "\n",
    "***V:*** Three companies, 'aa', ' a' and ' -', seem suspicious. For each of these companies it is clear that only one other company has the exact same location. Tus, we assume that the three suspicious companies are actually typos of the companies with the same location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***VI:*** We see that the company 'Brainsphere' is in two different cities in Portugal. So we count how many times the company occurs in each city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***VII:*** It is now clear that John from finances made a mistake since 'Brainspehere' is only found once in Monção. So we change the city from Monção to Braga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***VIII:*** We see that the company 'Thoughtmix' is in two different cities in Portugal. But after a quick google search we realize that John from finances once again made a mistake. Vila Fria is not a city but an area in Amadora and thus we changed it to Amadora. Note: We also removed the extra /t from Amadora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IX:*** Most of the companies now only have one specific country, one specific city and a NaN value in the 'city' column (except the company 'Flipstorm'). All the NaN values are switched to the specific city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***X:*** Now only the company 'Flipstorm' is messed up. This is easiely fixed. If the country is Greece the city should be Athens, and vice versa. If the country is France the city should be Nanterre, and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the 'price' column by removing rows without a price and make sure all currencies are in euro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.exchangeratesapi.io/history?start_at=2000-01-01&end_at=2019-10-08')\n",
    "json_response = r.json()\n",
    "\n",
    "rates = json_response['rates']\n",
    "\n",
    "real_dates = []\n",
    "\n",
    "for dates in rates:\n",
    "    real_dates.append(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-', 'nan', 'na', 'void']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removable = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    price = str(df.iloc[i,3])\n",
    "    \n",
    "    regex = re.search(r\"^\\-?\\d+\\.\\d{,3}\\€{1}\", price)\n",
    "    \n",
    "    if regex is None:\n",
    "        regex_new = re.search(r\"\\-?\\d+\\.\\d{,3}\", price)\n",
    "        \n",
    "        if regex_new is None:\n",
    "            removable.append(price)\n",
    "            \n",
    "list(set(removable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_price = []\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['price']) == True or row['price'] == '-' or row['price'] == 'na' or row['price'] == 'void':\n",
    "        remove_price.append(index)\n",
    "df.drop(remove_price, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://gist.githubusercontent.com/Fluidbyte/2973986/raw/b0d1722b04b0a737aade2ce6e055263625a0b435/Common-Currency.json')\n",
    "json_response = r.json()\n",
    "\n",
    "aDict = {}\n",
    "for r in json_response: \n",
    "    aDict[json_response[r]['symbol']] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_currency = []\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    try:\n",
    "        original_price = df.iloc[i,3]\n",
    "\n",
    "        regex = re.search(r\"^\\-?\\d+\\.\\d{,3}\\€{1}\", original_price)\n",
    "\n",
    "        if regex is not None:\n",
    "            price = float(df.iloc[i,3][:-1])\n",
    "            df.iloc[i,3] = round(price,2)\n",
    "\n",
    "        elif regex is None:\n",
    "            \n",
    "            non_digit = re.search(r'\\D',df.iloc[i,3])\n",
    "            non_digit = non_digit.group(0)\n",
    "            \n",
    "            date = str(df.iloc[i,4])\n",
    "            date = date[:-9]\n",
    "\n",
    "            currency = aDict[non_digit]\n",
    "            \n",
    "            while date not in real_dates:\n",
    "                date = pd.to_datetime(date)\n",
    "                date = date - pd.Timedelta(days=1)\n",
    "                date = str(date)\n",
    "                date = date[:-9]\n",
    "                \n",
    "            \n",
    "            rate = rates[date][currency]\n",
    "            \n",
    "  \n",
    "            price = float(df.iloc[i,3][1:]) / rate\n",
    "            df.iloc[i,3] = round(price,2)  \n",
    "            \n",
    "    except:\n",
    "        no_currency.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(no_currency, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company            object\n",
       "country            object\n",
       "city               object\n",
       "price             float64\n",
       "date       datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'] = pd.to_numeric(df['price'])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD TEXT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***II:*** We change the currency to euro in lines with '£', '$' and '¥' and for all the prices the currency symbol is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites used for currency:\n",
    "\n",
    "- Currency for £ to €: https://www.xe.com/currencyconverter/convert/?Amount=1&From=GBP&To=EUR visited: 3/10\n",
    "- Currency for $ to €: https://www.xe.com/currencyconverter/convert/?Amount=1&From=USD&To=EUR visited: 3/10\n",
    "- Currency for ¥ to €: https://www.xe.com/currencyconverter/convert/?Amount=1&From=JPY&To=EUR visited: 3/10\n",
    "\n",
    "***III***: We delete the lines with 'void', 'na', 'nan' and '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IV***: The data in the 'price' column is changed to float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SUCCESS!*** The currencies have been changed to euro and the data type is now float. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status.\n",
    "\n",
    "The data has been cleaned and is now ready for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_rows = len(df)\n",
    "start_rows - end_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, 17 rows have been removed from the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Enriching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common task for a data scientists is to combine or enrich data from internal sources with data available from external sources. The purpose of this can be either to fix issues with the data or to make it easier to derive insights from the data.\n",
    "\n",
    "In this part you should enrich your data with data from at least one external source. You may look to part 4 for some  inspiration as to what is required. Your solution should be automated, i.e., you can not ask the reader of your notebook to download any data manually. You should argue why and what you expect to achieve by the enrichments you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enrich our data set we have contructed a new CSV file with more columns of data. \n",
    "\n",
    "This data has been derived from The World Bank. \n",
    "\n",
    "## MANGLER MERE INFO: ARGUE, where is the data from etc.\n",
    "\n",
    "## QUESTION: you can not ask the reader of your notebook to download any data manually.???? ER DET FORKERT DET VI HAR GJORT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://api.worldbank.org/v2/sources/2/country/all/series/NY.GDP.MKTP.CD/data?per_page=500&format=json&mrnev=1')\n",
    "json_response = r.json()\n",
    "res = json_response['source']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_country = []\n",
    "list_value_GDP_dollar = []\n",
    "\n",
    "counter = 0\n",
    "for r in res:\n",
    "    inner_res = r.get('variable')\n",
    "    value = r.get('value')\n",
    "\n",
    "    list_value_GDP_dollar.append(value)\n",
    "    \n",
    "    for r in inner_res:\n",
    "        country = r.get('value')\n",
    "        list_country.append(country)\n",
    "        break\n",
    "\n",
    "unique_country = []\n",
    "for country in list_country:\n",
    "    if country not in unique_country:\n",
    "        unique_country.append(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.exchangeratesapi.io/latest?symbols=USD')\n",
    "json_response = r.json()\n",
    "\n",
    "rates = json_response['rates']['USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_value_GDP_euro = []\n",
    "for i in list_value_GDP_dollar:\n",
    "    list_value_GDP_euro.append(i*rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          country      gdp_euro\n",
      "0                      Arab World  3.055854e+12\n",
      "1          Caribbean small states  8.012120e+10\n",
      "2  Central Europe and the Baltics  1.792142e+12\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.DataFrame(list(zip(unique_country,list_value_GDP_euro)), columns =['country','gdp_euro'])\n",
    "print(new_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         company country       city   price                date      gdp_euro\n",
      "0  Chatterbridge   Spain  Barcelona  784.79 2016-01-02 00:01:05  1.566811e+12\n",
      "1  Chatterbridge   Spain  Barcelona  412.55 2016-01-02 04:51:55  1.566811e+12\n",
      "2  Chatterbridge   Spain  Barcelona  359.52 2016-01-02 07:20:59  1.566811e+12\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, new_df, on='country', how='inner')\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company             object\n",
       "country             object\n",
       "city                object\n",
       "price              float64\n",
       "date        datetime64[ns]\n",
       "gdp_euro           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20551"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Creating a database\n",
    "Storing data in a relational database has the advantages that it is persistent, fast to query, and it will be easier access for other employees at Weyland-Yutani.\n",
    "\n",
    "In this part you should:\n",
    "- Create a database and table(s) for the data\n",
    "- Insert data into the tables\n",
    "\n",
    "You may use SQLite locally to do this. You should argue why you choose to store your data the way you do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('transactions.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\"transactions\", conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x11a4e27a0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.execute('SELECT date(date) FROM transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20551"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Analyzing the data\n",
    "You are now ready to analyze the data. Your goal is to gain some actionable business insights to present to your boss. \n",
    "\n",
    "In this part, you should ask some questions and try to answer them based on the data. You should write SQL queries to retrieve the data. For each question, you should state why it is relevant and what you expect to find.\n",
    "\n",
    "To get you started, you should prepare answers to the following questions. You should add more questions.\n",
    "#### Who are the most profitable clients?\n",
    "Knowing which clients that generate the most revenue for the company will assist your boss in distributing customer service ressources.\n",
    "\n",
    "#### Are there any clients for which profit is declining?\n",
    "Declining profit from a specific client may indicate that the client is disatisfied with the product. Gaining a new client is often much more work than retaining one. Early warnings about declining profit may help your boss fighting customer churn.\n",
    "\n",
    "\n",
    "Remember, you are taking this to your new boss, so think about how you present the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "      <th>gdp_euro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8810</th>\n",
       "      <td>Zooxo</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>London</td>\n",
       "      <td>-727.47</td>\n",
       "      <td>2016-06-28 06:31:55</td>\n",
       "      <td>3.103773e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>-521.70</td>\n",
       "      <td>2017-09-09 04:05:49</td>\n",
       "      <td>1.566811e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15933</th>\n",
       "      <td>Ntags</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>-364.17</td>\n",
       "      <td>2018-01-27 14:43:38</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12214</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>-297.24</td>\n",
       "      <td>2016-09-22 18:01:17</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9808</th>\n",
       "      <td>Zooxo</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>London</td>\n",
       "      <td>-283.85</td>\n",
       "      <td>2018-04-18 03:51:50</td>\n",
       "      <td>3.103773e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>Flipstorm</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>-217.88</td>\n",
       "      <td>2016-01-05 12:21:25</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>Yozio</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "      <td>-185.05</td>\n",
       "      <td>2016-11-07 08:16:26</td>\n",
       "      <td>2.395298e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>-181.47</td>\n",
       "      <td>2016-01-25 09:38:31</td>\n",
       "      <td>1.566811e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12463</th>\n",
       "      <td>Ntags</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>-173.53</td>\n",
       "      <td>2016-10-23 14:27:46</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12994</th>\n",
       "      <td>Shufflebeat</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Porto</td>\n",
       "      <td>-168.28</td>\n",
       "      <td>2017-01-02 12:09:55</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16342</th>\n",
       "      <td>Brainsphere</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Braga</td>\n",
       "      <td>-158.51</td>\n",
       "      <td>2018-03-22 02:38:12</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17698</th>\n",
       "      <td>Shufflebeat</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Porto</td>\n",
       "      <td>-158.36</td>\n",
       "      <td>2018-09-23 21:50:08</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>Flipstorm</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "      <td>-158.02</td>\n",
       "      <td>2016-01-15 15:31:31</td>\n",
       "      <td>2.395298e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16151</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>-156.42</td>\n",
       "      <td>2018-02-25 18:09:20</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10488</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>-148.34</td>\n",
       "      <td>2016-02-08 20:41:09</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17303</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>-141.76</td>\n",
       "      <td>2018-07-27 03:20:28</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19864</th>\n",
       "      <td>Wordify</td>\n",
       "      <td>United States</td>\n",
       "      <td>New York</td>\n",
       "      <td>-135.65</td>\n",
       "      <td>2018-12-12 22:41:50</td>\n",
       "      <td>2.251482e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>Yozio</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "      <td>-133.25</td>\n",
       "      <td>2016-01-22 14:23:10</td>\n",
       "      <td>2.395298e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8166</th>\n",
       "      <td>Buzzbean</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Düsseldorf</td>\n",
       "      <td>-130.95</td>\n",
       "      <td>2018-02-20 10:02:16</td>\n",
       "      <td>4.390840e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17615</th>\n",
       "      <td>Brainsphere</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Braga</td>\n",
       "      <td>-121.28</td>\n",
       "      <td>2018-09-11 14:32:24</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>Eimbee</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>-119.86</td>\n",
       "      <td>2016-06-13 21:20:51</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6607</th>\n",
       "      <td>Rhycero</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>-119.83</td>\n",
       "      <td>2018-05-28 21:13:30</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20377</th>\n",
       "      <td>Bubblemix</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Asaka</td>\n",
       "      <td>-117.99</td>\n",
       "      <td>2016-05-29 16:23:37</td>\n",
       "      <td>5.461048e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>Zooxo</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>London</td>\n",
       "      <td>-117.01</td>\n",
       "      <td>2016-04-30 18:55:46</td>\n",
       "      <td>3.103773e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18119</th>\n",
       "      <td>Shufflebeat</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Porto</td>\n",
       "      <td>-109.47</td>\n",
       "      <td>2018-11-17 12:11:04</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4062</th>\n",
       "      <td>Twitterbeat</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>-106.89</td>\n",
       "      <td>2016-05-08 20:17:07</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20501</th>\n",
       "      <td>Kanoodle</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Niihama</td>\n",
       "      <td>-96.37</td>\n",
       "      <td>2018-05-09 03:07:40</td>\n",
       "      <td>5.461048e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10257</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>-95.08</td>\n",
       "      <td>2016-01-07 08:45:08</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8479</th>\n",
       "      <td>Buzzbean</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Düsseldorf</td>\n",
       "      <td>-92.97</td>\n",
       "      <td>2018-11-12 00:00:18</td>\n",
       "      <td>4.390840e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9674</th>\n",
       "      <td>Zooxo</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>London</td>\n",
       "      <td>-92.42</td>\n",
       "      <td>2018-01-19 04:07:00</td>\n",
       "      <td>3.103773e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15614</th>\n",
       "      <td>Brainsphere</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Braga</td>\n",
       "      <td>1735.89</td>\n",
       "      <td>2017-12-18 08:02:38</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15928</th>\n",
       "      <td>Shufflebeat</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Porto</td>\n",
       "      <td>1737.81</td>\n",
       "      <td>2018-01-27 02:33:47</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19143</th>\n",
       "      <td>Zoonder</td>\n",
       "      <td>United States</td>\n",
       "      <td>Boston</td>\n",
       "      <td>1738.66</td>\n",
       "      <td>2017-06-07 21:07:45</td>\n",
       "      <td>2.251482e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1743.18</td>\n",
       "      <td>2018-07-10 19:39:37</td>\n",
       "      <td>1.566811e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9005</th>\n",
       "      <td>Zooxo</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>London</td>\n",
       "      <td>1749.32</td>\n",
       "      <td>2016-11-19 07:02:39</td>\n",
       "      <td>3.103773e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>Eimbee</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>1756.72</td>\n",
       "      <td>2017-09-14 05:39:47</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12726</th>\n",
       "      <td>Brainsphere</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Braga</td>\n",
       "      <td>1772.29</td>\n",
       "      <td>2016-11-27 01:19:21</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14965</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>1773.72</td>\n",
       "      <td>2017-09-22 21:02:53</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11814</th>\n",
       "      <td>Shufflebeat</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Porto</td>\n",
       "      <td>1791.75</td>\n",
       "      <td>2016-08-06 08:11:35</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1795.82</td>\n",
       "      <td>2017-04-28 21:39:26</td>\n",
       "      <td>1.566811e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15698</th>\n",
       "      <td>Brainsphere</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Braga</td>\n",
       "      <td>1797.42</td>\n",
       "      <td>2017-12-29 08:14:04</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6967</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>1799.59</td>\n",
       "      <td>2018-09-19 20:20:36</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1800.82</td>\n",
       "      <td>2016-05-26 18:53:24</td>\n",
       "      <td>1.566811e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13023</th>\n",
       "      <td>Ntags</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>1802.74</td>\n",
       "      <td>2017-01-06 01:59:44</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>Twitterbeat</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>1812.14</td>\n",
       "      <td>2017-10-28 20:56:05</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1813.74</td>\n",
       "      <td>2017-04-15 19:26:09</td>\n",
       "      <td>1.566811e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15637</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>1839.54</td>\n",
       "      <td>2017-12-21 10:30:18</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17422</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>1853.15</td>\n",
       "      <td>2018-08-14 23:14:18</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7030</th>\n",
       "      <td>Twitterbeat</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>1861.57</td>\n",
       "      <td>2018-10-08 00:32:01</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13548</th>\n",
       "      <td>Ntags</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>1869.63</td>\n",
       "      <td>2017-03-19 15:05:58</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12542</th>\n",
       "      <td>Roodel</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Aranhas</td>\n",
       "      <td>1895.58</td>\n",
       "      <td>2016-11-04 01:23:52</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>Flipstorm</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "      <td>1901.39</td>\n",
       "      <td>2016-07-28 20:36:53</td>\n",
       "      <td>2.395298e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>Twitterbeat</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>1912.36</td>\n",
       "      <td>2016-12-21 13:36:34</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>Eimbee</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>1958.86</td>\n",
       "      <td>2017-06-09 07:28:03</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12079</th>\n",
       "      <td>Roodel</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Aranhas</td>\n",
       "      <td>1975.46</td>\n",
       "      <td>2016-09-08 05:17:17</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>Twitterbeat</td>\n",
       "      <td>France</td>\n",
       "      <td>Nanterre</td>\n",
       "      <td>2035.70</td>\n",
       "      <td>2018-11-26 02:42:16</td>\n",
       "      <td>3.051400e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>Flipstorm</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "      <td>2092.49</td>\n",
       "      <td>2016-09-07 13:31:12</td>\n",
       "      <td>2.395298e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11751</th>\n",
       "      <td>Ntags</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>2126.82</td>\n",
       "      <td>2016-07-29 01:30:35</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12095</th>\n",
       "      <td>Ntags</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Lisbon</td>\n",
       "      <td>2140.64</td>\n",
       "      <td>2016-09-09 04:47:05</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16460</th>\n",
       "      <td>Thoughtmix</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>Amadora</td>\n",
       "      <td>2174.74</td>\n",
       "      <td>2018-04-06 06:10:25</td>\n",
       "      <td>2.614437e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20551 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             company         country        city    price                date  \\\n",
       "8810           Zooxo  United Kingdom      London  -727.47 2016-06-28 06:31:55   \n",
       "901    Chatterbridge           Spain   Barcelona  -521.70 2017-09-09 04:05:49   \n",
       "15933          Ntags        Portugal      Lisbon  -364.17 2018-01-27 14:43:38   \n",
       "12214     Thoughtmix        Portugal     Amadora  -297.24 2016-09-22 18:01:17   \n",
       "9808           Zooxo  United Kingdom      London  -283.85 2018-04-18 03:51:50   \n",
       "3440       Flipstorm          France    Nanterre  -217.88 2016-01-05 12:21:25   \n",
       "2547           Yozio          Greece      Athens  -185.05 2016-11-07 08:16:26   \n",
       "38     Chatterbridge           Spain   Barcelona  -181.47 2016-01-25 09:38:31   \n",
       "12463          Ntags        Portugal      Lisbon  -173.53 2016-10-23 14:27:46   \n",
       "12994    Shufflebeat        Portugal       Porto  -168.28 2017-01-02 12:09:55   \n",
       "16342    Brainsphere        Portugal       Braga  -158.51 2018-03-22 02:38:12   \n",
       "17698    Shufflebeat        Portugal       Porto  -158.36 2018-09-23 21:50:08   \n",
       "1656       Flipstorm          Greece      Athens  -158.02 2016-01-15 15:31:31   \n",
       "16151     Thoughtmix        Portugal     Amadora  -156.42 2018-02-25 18:09:20   \n",
       "10488     Thoughtmix        Portugal     Amadora  -148.34 2016-02-08 20:41:09   \n",
       "17303     Thoughtmix        Portugal     Amadora  -141.76 2018-07-27 03:20:28   \n",
       "19864        Wordify   United States    New York  -135.65 2018-12-12 22:41:50   \n",
       "1678           Yozio          Greece      Athens  -133.25 2016-01-22 14:23:10   \n",
       "8166        Buzzbean         Germany  Düsseldorf  -130.95 2018-02-20 10:02:16   \n",
       "17615    Brainsphere        Portugal       Braga  -121.28 2018-09-11 14:32:24   \n",
       "4211          Eimbee          France    Nanterre  -119.86 2016-06-13 21:20:51   \n",
       "6607         Rhycero          France    Nanterre  -119.83 2018-05-28 21:13:30   \n",
       "20377      Bubblemix           Japan       Asaka  -117.99 2016-05-29 16:23:37   \n",
       "8723           Zooxo  United Kingdom      London  -117.01 2016-04-30 18:55:46   \n",
       "18119    Shufflebeat        Portugal       Porto  -109.47 2018-11-17 12:11:04   \n",
       "4062     Twitterbeat          France    Nanterre  -106.89 2016-05-08 20:17:07   \n",
       "20501       Kanoodle           Japan     Niihama   -96.37 2018-05-09 03:07:40   \n",
       "10257     Thoughtmix        Portugal     Amadora   -95.08 2016-01-07 08:45:08   \n",
       "8479        Buzzbean         Germany  Düsseldorf   -92.97 2018-11-12 00:00:18   \n",
       "9674           Zooxo  United Kingdom      London   -92.42 2018-01-19 04:07:00   \n",
       "...              ...             ...         ...      ...                 ...   \n",
       "15614    Brainsphere        Portugal       Braga  1735.89 2017-12-18 08:02:38   \n",
       "15928    Shufflebeat        Portugal       Porto  1737.81 2018-01-27 02:33:47   \n",
       "19143        Zoonder   United States      Boston  1738.66 2017-06-07 21:07:45   \n",
       "1328   Chatterbridge           Spain   Barcelona  1743.18 2018-07-10 19:39:37   \n",
       "9005           Zooxo  United Kingdom      London  1749.32 2016-11-19 07:02:39   \n",
       "5851          Eimbee          France    Nanterre  1756.72 2017-09-14 05:39:47   \n",
       "12726    Brainsphere        Portugal       Braga  1772.29 2016-11-27 01:19:21   \n",
       "14965     Thoughtmix        Portugal     Amadora  1773.72 2017-09-22 21:02:53   \n",
       "11814    Shufflebeat        Portugal       Porto  1791.75 2016-08-06 08:11:35   \n",
       "703    Chatterbridge           Spain   Barcelona  1795.82 2017-04-28 21:39:26   \n",
       "15698    Brainsphere        Portugal       Braga  1797.42 2017-12-29 08:14:04   \n",
       "6967          Avaveo          France    Nanterre  1799.59 2018-09-19 20:20:36   \n",
       "229    Chatterbridge           Spain   Barcelona  1800.82 2016-05-26 18:53:24   \n",
       "13023          Ntags        Portugal      Lisbon  1802.74 2017-01-06 01:59:44   \n",
       "5989     Twitterbeat          France    Nanterre  1812.14 2017-10-28 20:56:05   \n",
       "682    Chatterbridge           Spain   Barcelona  1813.74 2017-04-15 19:26:09   \n",
       "15637     Thoughtmix        Portugal     Amadora  1839.54 2017-12-21 10:30:18   \n",
       "17422     Thoughtmix        Portugal     Amadora  1853.15 2018-08-14 23:14:18   \n",
       "7030     Twitterbeat          France    Nanterre  1861.57 2018-10-08 00:32:01   \n",
       "13548          Ntags        Portugal      Lisbon  1869.63 2017-03-19 15:05:58   \n",
       "12542         Roodel        Portugal     Aranhas  1895.58 2016-11-04 01:23:52   \n",
       "2348       Flipstorm          Greece      Athens  1901.39 2016-07-28 20:36:53   \n",
       "4971     Twitterbeat          France    Nanterre  1912.36 2016-12-21 13:36:34   \n",
       "5532          Eimbee          France    Nanterre  1958.86 2017-06-09 07:28:03   \n",
       "12079         Roodel        Portugal     Aranhas  1975.46 2016-09-08 05:17:17   \n",
       "7194     Twitterbeat          France    Nanterre  2035.70 2018-11-26 02:42:16   \n",
       "2437       Flipstorm          Greece      Athens  2092.49 2016-09-07 13:31:12   \n",
       "11751          Ntags        Portugal      Lisbon  2126.82 2016-07-29 01:30:35   \n",
       "12095          Ntags        Portugal      Lisbon  2140.64 2016-09-09 04:47:05   \n",
       "16460     Thoughtmix        Portugal     Amadora  2174.74 2018-04-06 06:10:25   \n",
       "\n",
       "           gdp_euro  \n",
       "8810   3.103773e+12  \n",
       "901    1.566811e+12  \n",
       "15933  2.614437e+11  \n",
       "12214  2.614437e+11  \n",
       "9808   3.103773e+12  \n",
       "3440   3.051400e+12  \n",
       "2547   2.395298e+11  \n",
       "38     1.566811e+12  \n",
       "12463  2.614437e+11  \n",
       "12994  2.614437e+11  \n",
       "16342  2.614437e+11  \n",
       "17698  2.614437e+11  \n",
       "1656   2.395298e+11  \n",
       "16151  2.614437e+11  \n",
       "10488  2.614437e+11  \n",
       "17303  2.614437e+11  \n",
       "19864  2.251482e+13  \n",
       "1678   2.395298e+11  \n",
       "8166   4.390840e+12  \n",
       "17615  2.614437e+11  \n",
       "4211   3.051400e+12  \n",
       "6607   3.051400e+12  \n",
       "20377  5.461048e+12  \n",
       "8723   3.103773e+12  \n",
       "18119  2.614437e+11  \n",
       "4062   3.051400e+12  \n",
       "20501  5.461048e+12  \n",
       "10257  2.614437e+11  \n",
       "8479   4.390840e+12  \n",
       "9674   3.103773e+12  \n",
       "...             ...  \n",
       "15614  2.614437e+11  \n",
       "15928  2.614437e+11  \n",
       "19143  2.251482e+13  \n",
       "1328   1.566811e+12  \n",
       "9005   3.103773e+12  \n",
       "5851   3.051400e+12  \n",
       "12726  2.614437e+11  \n",
       "14965  2.614437e+11  \n",
       "11814  2.614437e+11  \n",
       "703    1.566811e+12  \n",
       "15698  2.614437e+11  \n",
       "6967   3.051400e+12  \n",
       "229    1.566811e+12  \n",
       "13023  2.614437e+11  \n",
       "5989   3.051400e+12  \n",
       "682    1.566811e+12  \n",
       "15637  2.614437e+11  \n",
       "17422  2.614437e+11  \n",
       "7030   3.051400e+12  \n",
       "13548  2.614437e+11  \n",
       "12542  2.614437e+11  \n",
       "2348   2.395298e+11  \n",
       "4971   3.051400e+12  \n",
       "5532   3.051400e+12  \n",
       "12079  2.614437e+11  \n",
       "7194   3.051400e+12  \n",
       "2437   2.395298e+11  \n",
       "11751  2.614437e+11  \n",
       "12095  2.614437e+11  \n",
       "16460  2.614437e+11  \n",
       "\n",
       "[20551 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_by_price = df.sort_values('price')\n",
    "sort_by_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Performance\n",
    "Your boss is very impressed with what you have achieved in less than two weeks, and he would like to take your idea of storing the customer and sales data in a relational database to production. However, John is concerned that the solution will not scale. His experience is telling him that you will see many occurrences of the following queries.\n",
    "\n",
    "- Show all sales to company X between time $t_1$ and time $t_2$\n",
    "- Show the latest X sales in the database\n",
    "- Show total sales per company per day\n",
    "\n",
    "Show that Johns concern is not justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
